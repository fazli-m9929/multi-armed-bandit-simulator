# Multi-Armed Bandit Simulator

A clean, modular, lightweight simulator for testing and comparing different **multi-armed bandit** strategies, implemented in Python. It includes key exploration methods like **Îµ-greedy**, **UCB**, and **Gradient Bandit**, along with plotting and performance evaluation tools.

---

## ğŸš€ Overview

This project simulates the **k-armed bandit problem**, a cornerstone of reinforcement learning, to analyze how agents balance **exploration vs. exploitation**.

You can:

* Simulate stochastic reward environments.
* Test multiple agent strategies with or without decay mechanisms.
* Track reward evolution and optimal action percentages.
* Visualize and compare performance.

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ bandit/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ env.py                  # KArmedBandit environment class
â”‚   â”œâ”€â”€ simulator.py            # Experiment runner for agents
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ epsilon_greedy.py   # Îµ-greedy strategy (fixed, decayed, optimistic, etc.)
â”‚   â”‚   â”œâ”€â”€ ucb.py              # Upper Confidence Bound (UCB) agent
â”‚   â”‚   â””â”€â”€ gradient_bandit.py  # Policy gradient-based agent
â”‚   â””â”€â”€ __pycache__/
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ main.ipynb              # Interactive tests and visualization
â”œâ”€â”€ pyproject.toml              # uv project configuration
â”œâ”€â”€ README.md
â””â”€â”€ uv.lock                     # dependency lock file
```

---

## âš™ï¸ Environment

The **KArmedBandit** simulates a stochastic reward process:

```python
KArmedBandit(k=10, mean_range=(-2, 2), std_range=(0.5, 1.5))
```

Each arm has its own Gaussian reward distribution. You can call:

```python
reward = env.pull(arm)
```

---

## ğŸ§  Agents Tested

The following configurations were tested in `main.ipynb`:

```python
agent_configs = [
    {"type": "epsilon", "k": 10, "epsilon": 0.3},
    {"type": "epsilon", "k": 10, "epsilon": 0.01},
    {"type": "epsilon", "k": 10, "epsilon": 0.3, "initial_value": 5.0},
    {"type": "epsilon", "k": 10, "epsilon": 0.3, "step_size": 0.1},
    {"type": "epsilon", "k": 10, "epsilon": 0.6, "decay_type": "exponential", "decay_rate": 0.005, "epsilon_min": 0.01},
    {"type": "ucb", "k": 10, "c": 2.0},
    {"type": "gradient", "k": 10, "step_size": 0.1, "use_baseline": True},
]
```

All agents were instantiated dynamically and tested under identical environments for fair comparison.

---

## ğŸ“Š Results Overview

Experiments were run with `steps=1000` and `runs=200`.

* **Exponential decay** Îµ-greedy showed smoother convergence and strong balance between exploration and exploitation.
* **Optimistic initialization** boosted early exploration but stabilized slower.
* **UCB** achieved consistent results but was more conservative.
* **Gradient Bandit** converged cleanly, favoring stable reward arms over time.

---

## ğŸ§© Visualization

Plots included:

* Average reward over time (smoothed)
* Optimal action percentage

Example (from the notebook):

```python
plot_results(agents, avg_rewards, optimal_actions)
```

---

## ğŸš€ Getting Started

```bash
uv sync
uv run notebooks/main.ipynb
```

Or open the notebook in VS Code / Jupyter and explore interactively.

---

## ğŸ§¾ License

MIT License â€” free to use, modify, and extend.

## Author

**MohammadReza Fazli**
